from pathlib import Path
import configparser
import urllib.request
import zipfile
import datetime
import pandas as pd
import logging
import sys
from datetime import datetime
import inspect
import numpy as np
import matplotlib.pyplot as plt
from cycler import cycler
import traceback


############ LOGGER ############


class ExitOnCriticalHandler(logging.Handler):
    """
    Custom handler that immediately terminates execution
    when a CRITICAL log is emitted.
    """

    def emit(self, record):
        try:
            if record.levelno >= logging.CRITICAL:
                logging.shutdown()

                # Print final CRITICAL message cleanly to stderr
                sys.stderr.write(
                    f"\n\n[CRITICAL] {record.getMessage()}\n"
                )
                if record.exc_info:
                    traceback.print_exception(*record.exc_info)

                sys.stderr.write("\nExecution halted due to critical error.\n")
                sys.stderr.flush()

                # Exit with a nonzero status code
                sys.exit(1)

        except Exception:
            # Fallback if logging itself fails
            sys.stderr.write("\n[CRITICAL] Failed to terminate cleanly.\n")
            sys.exit(1)


def setup_logger(name: str, level=logging.INFO, log_root: Path | str = "./logs") -> logging.Logger:
    """
    Set up a logger that writes to both console and file.
    Applies globally to all modules in the project.
    """
    # --- Root logger ---
    root_logger = logging.getLogger()
    root_logger.setLevel(level)

    # Prevent adding duplicate handlers
    if not root_logger.handlers:
        formatter = logging.Formatter(
            "[%(asctime)s] %(levelname)s - %(name)s - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )

        # --- Console handler ---
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        root_logger.addHandler(console_handler)

        # --- File handler ---
        log_root = Path(log_root)
        date_subfolder = datetime.now().strftime("%Y-%m-%d")
        log_dir = log_root / date_subfolder
        log_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        log_filename = f"{name}_{timestamp}.log"
        log_path = log_dir / log_filename

        file_handler = logging.FileHandler(log_path, encoding="utf-8")
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)

        # stop execution on critical
        root_logger.addHandler(ExitOnCriticalHandler())

        root_logger.info(f"Logger initialized. Writing logs to {log_path.resolve()}")

    # Return a named child logger
    return logging.getLogger(name)


############ CONFIG ############


def load_config(logger: logging.Logger, config_path: str | Path = "config.ini") -> configparser.ConfigParser:
    """
    Import the config file.
    """

    logger.info(f"Gathering info from config {config_path}")

    config = configparser.ConfigParser()
    config.read(config_path)

    # Lettura variabili globali
    current_events = [
        x.strip() for x in config["global_variables"]["current_events"].split(",")
    ]
    multivenue = [
        x.strip() for x in config["global_variables"]["multivenue"].split(",")
    ]

    config.current_events = current_events
    config.multivenue = multivenue
    config.year = int(config["global_variables"]["year"])
    config.country = config["global_variables"]["country"]
    config.nationality = config["global_variables"]["nationality"]
    config.championship_type = config["global_variables"]["championship_type"]

    if config.has_section("plot"):
        fig_size = tuple(
            float(x.strip()) for x in config["plot"]["figure_size"].split(",")
        )
        config.figure_size = fig_size
        config.dpi = int(config["plot"]["dpi"])
    else:
        config.figure_size = (12, 6)
        config.dpi = 100

    return config


############ PLOTS ############


def set_plot_style(config: configparser.ConfigParser, logger: logging.Logger | None = None):
    """
    Apply consistent matplotlib styling based on config.ini [plot] section.
    """
    try:
        cfg = config["plot"]

        # --- Parse color cycle ---
        colors = [c.strip() for c in cfg.get("color_cycle", "").split(",") if c.strip()]

        # --- Parse figure size safely ---
        try:
            fig_size = eval(cfg.get("figure_size", "(8, 5)"))
        except Exception:
            fig_size = (8, 5)
            if logger:
                logger.warning("Invalid 'figure_size' in config.ini, using default (8, 5)")

        # --- Update matplotlib rcParams ---
        plt.rcParams.update({
            # --- Figure ---
            "figure.figsize": fig_size,
            "figure.dpi": cfg.getint("dpi", 150),
            "savefig.dpi": cfg.getint("save_dpi", 300),
            "savefig.bbox": cfg.get("bbox", "tight"),
            "figure.autolayout": cfg.getboolean("autolayout", True),

            # --- Axes ---
            "axes.titlesize": cfg.getint("axes_title_size", 14),
            "axes.labelsize": cfg.getint("axes_label_size", 12),
            "axes.grid": cfg.getboolean("axes_grid", True),
            "axes.edgecolor": cfg.get("axes_edge_color", "gray"),
            "grid.alpha": cfg.getfloat("grid_alpha", 0.4),
            "grid.linestyle": cfg.get("grid_linestyle", ":"),
            "grid.color": cfg.get("grid_color", "lightgray"),

            # --- Font ---
            "font.family": cfg.get("font_family", "sans-serif"),
            "font.sans-serif": [x.strip() for x in cfg.get("font_sans_serif", "DejaVu Sans").split(",")],
            "font.size": cfg.getint("font_size", 11),
            "text.color": cfg.get("text_color", "black"),

            # --- Ticks and Legend ---
            "xtick.labelsize": cfg.getint("tick_label_size", 10),
            "ytick.labelsize": cfg.getint("tick_label_size", 10),
            "legend.fontsize": cfg.getint("legend_font_size", 10),
            "legend.frameon": cfg.getboolean("legend_frame", False),

            # --- Lines & Colors ---
            "lines.linewidth": cfg.getfloat("line_width", 2.0),
            "axes.prop_cycle": cycler("color", colors or ["#1f77b4", "#ff7f0e", "#2ca02c"]),
        })

        if logger:
            logger.info("Matplotlib plotting style successfully applied from config.ini")

    except Exception as e:
        if logger:
            logger.warning(f"Could not set plotting style: {e}", exc_info=True)


############ UTILS ############


def get_database_dir(config: configparser.ConfigParser) -> Path:
    """
    Return the path to the database_export directory, creating it if needed.
    """

    try:
        db_dir = Path(config["paths"]["database_export_dir"]).resolve()
    except KeyError:
        raise KeyError("Missing [paths] -> database_export_dir in config.ini")

    db_dir.mkdir(parents=True, exist_ok=True)

    return db_dir


def update_data(config: configparser.ConfigParser, logger: logging.Logger | None = None) -> None:
    """
    Download the latest WCA export if not already downloaded today.
    Uses logger if provided.
    """
    db_dir = get_database_dir(config)
    meta_file = db_dir / "last_update.txt"
    url = config["url"]["wca_export_url"]

    today_str = datetime.now().date().isoformat()

    # Check if already updated today
    if meta_file.exists():
        last_update = meta_file.read_text().strip()
        if last_update == today_str:
            if logger:
                logger.info(f"Data already up to date (last update: {last_update})")
            else:
                print(f"Data already up to date (last update: {last_update})")
            return

    if logger:
        logger.info(f"Downloading new WCA export from {url} ...")
    else:
        print(f"Downloading new WCA export from {url} ...")

    zip_path, _ = urllib.request.urlretrieve(url)
    with zipfile.ZipFile(zip_path, "r") as f:
        f.extractall(db_dir)

    meta_file.write_text(today_str)
    if logger:
        logger.info(f"WCA data updated successfully on {today_str} in {db_dir}")
    else:
        print(f"WCA data updated successfully on {today_str} in {db_dir}")


def read_table(table_name: str, config: configparser.ConfigParser, logger: logging.Logger | None = None) -> pd.DataFrame:
    """
    Reads a WCA table (.tsv) based on config mappings.
    Logs messages if logger is provided.
    """
    db_dir = get_database_dir(config)

    if not config.has_section("tables"):
        if logger:
            logger.critical("Missing [tables] section in config.ini")
        else:
            raise KeyError("Missing [tables] section in config.ini")

    tables_map = dict(config.items("tables"))
    if table_name not in tables_map:
        if logger:
            logger.critical(f"Table '{table_name}' not found in [tables] section of config.ini")
        else:
            raise KeyError(f"Table '{table_name}' not found in [tables] section of config.ini")

    file_name = tables_map[table_name]
    file_path = db_dir / file_name

    if not file_path.exists():
        if logger:
            logger.critical(f"File not found: {file_path}. You may need to run update_data().")
        else:
            raise FileNotFoundError(f"File not found: {file_path}. You may need to run update_data().")

    df = pd.read_csv(file_path, sep="\t", low_memory=False)

    if logger:
        logger.info(f"Loaded '{table_name}' from {file_path.name} ({len(df):,} rows)")
    else:
        print(f"Loaded '{table_name}' from {file_path.name} ({len(df):,} rows)")

    return df


def process_tables(db_tables: dict[str, pd.DataFrame], config: configparser.ConfigParser, logger: logging.Logger) -> dict[str, pd.DataFrame]:
    """
    Perform common operations on db_tables. e.g. Merges the 'results' and 'competitions' tables on 'competitionId' and filters rows to include only competitors from the configured country.

    Parameters
    ----------
    db_tables : dict[str, pd.DataFrame]
        Dictionary containing loaded WCA .tsv tables.
    config : ConfigParser
        Configuration object loaded with load_config().
    logger : logging.Logger, optional

    Returns
    -------
    dict[str, pd.DataFrame]
    """

    logger.info("Preprocessing tables...")

    # --- Merge results with competitions + filter for nationality ---
    try:  
        # Rename column in competitions
        db_tables["competitions"] = db_tables["competitions"].rename(columns = {'name':'competitionName'})

        logger.info("Renamed column name to competitionName in competitions dataframe")

        # Filter for nationality
        nationality_filter = config.nationality
        results_filtered = db_tables["results"].query("personCountryId == @nationality_filter")
        
        df = (
            results_filtered
            .merge(
                db_tables["competitions"],
                left_on="competitionId",
                right_on="id",
                how="left",
                validate = "m:1"
            )
            .drop('id', axis=1)
            .merge(
                db_tables["rounds"][['id','rank']],
                how='left',
                left_on='roundTypeId',
                right_on='id'
            )
            .drop('id', axis=1)
        )

        df['date'] = pd.to_datetime(df[['year','month','day']])

        db_tables["results_nationality"] = df

        # Filter for host country
        country_filter = config.country
        competitions_filtered = db_tables["competitions"].query("countryId == @country_filter")

        df = (
            db_tables["results"]
            .merge(
                competitions_filtered,
                left_on="competitionId",
                right_on="id",
                how="inner",
                validate = "m:1"
            )
            .drop('id', axis=1)
            .merge(
                db_tables["rounds"][['id','rank']],
                how='left',
                left_on='roundTypeId',
                right_on='id'
            )
            .drop('id', axis=1)
        )

        db_tables["results_country"] = df

        logger.info(f"Created 'results_nationality' from results+competitions with only competitors from country = {nationality_filter}.")
        logger.info(f"Created 'results_country' from results+competitions with only competitions from country = {country_filter}.")

    except Exception as e:
        logger.critical(f"Error during results/competitions merge: {e}", exc_info=True)
    

    # --- Extract list of championships ---
    try:
        config.nats = (
            list(
                db_tables["championships"][db_tables["championships"]['championship_type'] == config.championship_type]['competition_id']
            )
        )
        logger.info("extracted list of national championships")

    except Exception as e:
        logger.critical(f"Error during the listing of championships for championship type {config.championship_type}: {e}", exc_info=True)


    # --- Merge persons with ranks single ---
    try:
        db_tables["ranks_single"] = (
            db_tables["ranks_single"]
            .merge(
                db_tables["persons"][['id','name','countryId']], 
                how='left', 
                left_on='personId', 
                right_on='id'
            )
            .drop('id', axis=1)
        )
        logger.info("Merged persons with ranks_single")

    except Exception as e:
        logger.critical(f"Error during ranks_single/persons merge: {e}", exc_info=True)


    # --- Merge persons with ranks average ---
    try:
        db_tables["ranks_average"] = (
            db_tables["ranks_average"]
            .merge(
                db_tables["persons"][['id','name','countryId']], 
                how='left', 
                left_on='personId', 
                right_on='id'
            )
            .drop('id', axis=1)
        )
        logger.info("Merged persons with ranks_average")

    except Exception as e:
        logger.critical(f"Error during ranks_average/persons merge: {e}", exc_info=True)

    # Useful country lists
    config.countries = (
        list(
            db_tables["competitions"]['countryId'].drop_duplicates()
        )
    )
    config.real_countries = [x for x in config.countries if x not in config.multivenue]

    return db_tables


def export_data(results: dict, figures: dict | None, section_name: str, config: configparser.ConfigParser, logger: logging.Logger | None = None) -> dict:
    """
    Export module results: Excel + optional figures.

    Parameters
    ----------
    results : dict
        Dictionary {sheet_name: DataFrame} for Excel sheets.
    figures : dict, optional
        Dictionary {figure_name: matplotlib.figure.Figure} to save.
    section_name : str
        Name of the module/section.
    config : ConfigParser
        Loaded config.ini object.
    logger : logging.Logger, optional
        Logger for messages.

    """

    # --- Timestamp ---
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")

    # --- Output folder ---
    try:
        base_output_dir = Path(config["paths"]["output_dir"])
    except KeyError:
        base_output_dir = Path("./output")
        if logger:
            logger.warning("Missing [paths]->output_dir in config.ini. Using './output'.")

    module_dir = base_output_dir / section_name
    module_dir.mkdir(parents=True, exist_ok=True)

    # --- Excel ---
    try:
        template = config["output"].get("excel_template", "{section_name}_{timestamp}.xlsx")
    except KeyError:
        template = "{section_name}_{timestamp}.xlsx"

    excel_file = module_dir / template.format(section_name=section_name, timestamp=timestamp)
    with pd.ExcelWriter(excel_file, engine="openpyxl") as writer:
        for sheet_name, df in results.items():
            df.to_excel(writer, sheet_name=sheet_name[:31], index=False)

    if logger:
        logger.info(f"{section_name} results exported to Excel: {excel_file.resolve()}")

    # --- Figures ---
    if figures:
        try:
            figures_sub = config["output"]["figures_subfolder"]
        except KeyError:
            figures_sub = "figures"

        fig_dir = module_dir / figures_sub
        fig_dir.mkdir(exist_ok=True)

        for fig_name, fig in figures.items():
            fig_path = fig_dir / f"{fig_name}_{timestamp}.png"
            fig.savefig(fig_path)
            plt.close(fig)
            if logger:
                logger.info(f"Figure saved to {fig_path.resolve()}")


############ Functions ############


def truncate(num: float, n: int) -> float:
    """
    Truncate a number to a given number of decimal places.

    The WCA uses n=2 for all officially recorded times and 
    rounds for averages.

    Parameters
    ----------
    num : float
        The number to truncate.
    n : int
        The number of decimal places to keep.

    Returns
    -------
    float
        The truncated number, or NaN if the input is NaN.
    """
    if np.isnan(num):
        return np.nan

    integer = int(num * (10 ** n)) / (10 ** n)
    return float(integer)

def timeconvert(x: float) -> str:
    """
    Convert a WCA time in centiseconds to a human-readable string.

    Converts a time value (in centiseconds) to the format:
    - 'SS.CC' if under one minute
    - 'M:SS.CC' if one minute or more

    Parameters
    ----------
    x : float
        The time value in centiseconds.

    Returns
    -------
    str
        The formatted time string.
    """
    if x < 6000:
        return f"{x / 100:.2f}"
    else:
        a = x % 6000
        if a < 1000:
            return f"{int(x / 6000)}:0{a / 100:.2f}"
        return f"{int(x / 6000)}:{a / 100:.2f}"
